---
title: "Case Study"
author: "Pratheek Eravelli & Kanav Bhatnagar"
date: "3/26/2022"
output: html_document
---

# Project Setup

We will use the following packages for this homework assignment.  We will also read in data from the csv file.

```{r setup}
library(ggplot2)
library(lmtest)
library(MASS)
prostate = read.table('prostate.txt', header = FALSE)
```

Now, we add labels to the columns.

```{r adding_labels}
colnames(prostate) = c('ID', 'psalevel', 'cancervolume', 'prostateweight', 
                       'age', 'hyperplasia', 'svi', 'capsular', 'gleason')
head(prostate)
```

# Analysis

## Building the initial model

First, we build a regression model with the required variables, and look at the summary statistics.

```{r lm}
prostate_lm = lm(psalevel ~ cancervolume + age + hyperplasia + svi + capsular + gleason,
                 data = prostate)
summary(prostate_lm)
```

## Manual model selection

We can see in the summary for the linear model that capsular penetration, i.e. `capsular` is the least significant variable since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var1}
prostate_lm_reduced_1 = lm(psalevel ~ cancervolume + age + hyperplasia + svi + gleason,
                         data = prostate)
summary(prostate_lm_reduced_1)
```

Again, we notice that patient age, i.e. `age` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var2}
prostate_lm_reduced_2 = lm(psalevel ~ cancervolume + hyperplasia + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_2)
```

Now, we notice that the amount of benign prostatic hyperplasia, i.e. `hyperplasia` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var3}
prostate_lm_reduced_3 = lm(psalevel ~ cancervolume + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_3)
```

Finally, we notice that the Gleason score, i.e. `gleason` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var4}
prostate_lm_reduced_4 = lm(psalevel ~ cancervolume + svi,
                           data = prostate)
summary(prostate_lm_reduced_4)
```

We see that all the variables in the model have a *p*-value less than 0.05, which means that their slopes are all statistically significant, and they contribute to explaining the variation in the response.

Thus, the final model we have is `psalevel =` `r prostate_lm_reduced_4$coefficients[1]` + `r prostate_lm_reduced_4$coefficients[2]` $\cdot$ `cancervolume` + `r prostate_lm_reduced_4$coefficients[3]` $\cdot$ `svi`.

\newpage

## Unusual observations

### High Leverage Points
We will first look at identifying high leverage points. 
```{r high_leverage}
n = dim(prostate)[1] # Sample Size 
chosen_model = prostate_lm_reduced_4
p = length(variable.names(chosen_model)) # Predictors plus intercepts.

prostate_leverages = lm.influence(chosen_model)$hat

prostate_high_leverage = prostate_leverages[prostate_leverages > 2*p/n]
prostate_high_leverage



```
We now have the following high leverage points at our disposal.
We now need to find which of the following high leverage points can be considered "bad" high leverage points and which can be considered "good".
```{r}


  IQR_y = IQR(prostate$psalevel)

QT1_y = quantile(prostate$psalevel, 0.25)
QT3_y = quantile(prostate$psalevel, 0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y, upper_lim_y)
vector_lim_y

```
```{r}
high_lev_lower = prostate_high_leverage[prostate_high_leverage < vector_lim_y[1]]
high_lev_upper = prostate_high_leverage[prostate_high_leverage > vector_lim_y[2]]

leverage_range = rbind(high_lev_lower, high_lev_upper)
leverage_range

```

From the output above we can observe that none of the high leverage points can be considered "bad" high leverage points. 


### Outliers 

Now let us discuss OUTLIERS in the data:

1. We must find the studentized residuals.
2. Compute the bonferroni critical value using the number of observations and the number of parameters.
3. See if any of the residuals are greater than the absolute values of the bonferroni critical value.
4. If any are, then these are deemed as outliers in the data.

```{r outliers}
psa_residuals = rstudent(chosen_model)

bonferroni_cv = qt(0.05/(2*n), n - p - 1)

psa_residuals_sorted = sort(abs(psa_residuals), decreasing = TRUE)[1:10]

psa_residuals_sorted
print("Bonferroni Critical Values:")
bonferroni_cv
```
From the above analysis we can see that only points 96 and 97 of the original data set are considered outliers. 


### Influential Points
Now let us discuss influential points in the data:
We must calculate the cooks distance for each point and see if there are any points with a distance greater than 1.
```{r cooks_distance}
prostate_cooks = cooks.distance(chosen_model)
sort(prostate_cooks, decreasing = TRUE)[1:10]

```
\newline
From the above analysis we can see that point 97 is all three things.
Point 97 is a high leverage point, an outlier, and it is a highly influential points.

This is the only point that has this characteristic in the finally selected model. 
We will not be removing this point but we will think about it in the context of model assumptions. 

\newpage
## Model assumptions

### Constant Variance
We will now look at the constant variance assumption of the reduced model. 


```{r constant variance}
plot(chosen_model, which = 1)


```
The values of the residuals do not appear to be constant along the 0 line. It appears that 
the variance increases as we move along the x axis which is not good for the constant variance assumption.

We must now prove using *bp* test that we are operating under constant variance  

```{r }
bptest(chosen_model)
```
Because we have a p-value less than 0.05 we reject the assumption of constant variance for this data. 





