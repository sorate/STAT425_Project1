---
title: "Case Study"
author: "Pratheek Eravelli & Kanav Bhatnagar"
date: "3/26/2022"
output: html_document
---

# Project Setup

We will use the following packages for this homework assignment.  We will also read in data from the csv file.

```{r setup}
library(ggplot2)
prostate = read.table('prostate.txt', header = FALSE)
```

Now, we add labels to the columns.

```{r adding_labels}
colnames(prostate) = c('ID', 'psalevel', 'cancervolume', 'prostateweight', 
                       'age', 'hyperplasia', 'svi', 'capsular', 'gleason')
head(prostate)
```

# Analysis

## Building the initial model

First, we build a regression model with the required variables, and look at the summary statistics.

```{r lm}
prostate_lm = lm(psalevel ~ cancervolume + age + hyperplasia + svi + capsular + gleason,
                 data = prostate)
summary(prostate_lm)
```

## Manual model selection

We can see in the summary for the linear model that capsular penetration, i.e. `capsular` is the least significant variable since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var1}
prostate_lm_reduced_1 = lm(psalevel ~ cancervolume + age + hyperplasia + svi + gleason,
                         data = prostate)
summary(prostate_lm_reduced_1)
```

Again, we notice that patient age, i.e. `age` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var2}
prostate_lm_reduced_2 = lm(psalevel ~ cancervolume + hyperplasia + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_2)
```

Now, we notice that the amount of benign prostatic hyperplasia, i.e. `hyperplasia` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var3}
prostate_lm_reduced_3 = lm(psalevel ~ cancervolume + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_3)
```

Finally, we notice that the Gleason score, i.e. `gleason` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var4}
prostate_lm_reduced_4 = lm(psalevel ~ cancervolume + svi,
                           data = prostate)
summary(prostate_lm_reduced_4)
```

We see that all the variables in the model have a *p*-value less than 0.05, which means that their slopes are all statistically significant, and they contribute to explaining the variation in the response.

Thus, the final model we have is `psalevel =` `r prostate_lm_reduced_4$coefficients[1]` + `r prostate_lm_reduced_4$coefficients[2]` $\cdot$ `cancervolume` + `r prostate_lm_reduced_4$coefficients[3]` $\cdot$ `svi`.

\newpage

## Unusual observations

We will first look at identifying high leverage points. 
```{r high_leverage}
n = dim(prostate)[1] # Sample Size 
chosen_model = prostate_lm_reduced_4
p = length(variable.names(chosen_model)) # Predictors plus intercepts.

prostate_leverages = lm.influence(chosen_model)$hat

prostate_high_leverage = prostate_leverages[prostate_leverages > 2*p/n]
prostate_high_leverage



```
We now have the following high leverage points at our disposal.
We now need to find which of the following high leverage points can be considered "bad" high leverage points and which can be considered "good".
```{r}


  IQR_y = IQR(prostate$psalevel)

QT1_y = quantile(prostate$psalevel, 0.25)
QT3_y = quantile(prostate$psalevel, 0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y, upper_lim_y)
vector_lim_y

```
```{r}
high_lev_lower = prostate_high_leverage[prostate_high_leverage < vector_lim_y[1]]
high_lev_upper = prostate_high_leverage[prostate_high_leverage > vector_lim_y[2]]

leverage_range = rbind(high_lev_lower, high_lev_upper)
leverage_range

```

From the output above we can observe that none of the high leverage points can be considered "bad" high leverage points. 



Now let us discuss OUTLIERS in the data:
(1) We must find sudentized residuals residuals
(2) Compute the bonferroni critical value
(3) See if the residuals are greater than the absolute values of the conferroni critical value.
(4) If any are then these are deemed as outliers in the data.
```{r outliers}
psa_residuals = rstudent(chosen_model)

bonferroni_cv = qt(0.05/(2*n), n - p - 1)

psa_residuals_sorted = sort(abs(psa_residuals), decreasing = TRUE)[1:10]

psa_residuals_sorted
print("Bonferroni Critical Values:")
bonferroni_cv
```
From the above analysis we can see that only points 96 and 97 of the original data set are considered outliers. 


Now let us discuss influential points in the data:
(1)We must calculate the cooks distance for each point and see if there are any points with a distance greater than 1.
```{r cooks_distance}
prostate_cooks = cooks.distance(chosen_model)
sort(prostate_cooks, decreasing = TRUE)[1:10]

plot(prostate_cooks)

```
From the above analysis we can see that point 97 is all three things.
Point 97 is a high leverage point, an outlier, and it is a highly influential points.

This is the only point that has this characteristic in the actual model. 

## Model assumptions

