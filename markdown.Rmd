---
title: "Case Study"
author: "Pratheek Eravelli & Kanav Bhatnagar"
date: "3/26/2022"
output: pdf_document
---

# Project Setup

We will use the following packages for this homework assignment.  We will also read in data from the csv file.

```{r setup}
library(ggplot2)
prostate = read.table('prostate.txt', header = FALSE)
```

Now, we add labels to the columns.

```{r adding_labels}
colnames(prostate) = c('ID', 'psalevel', 'cancervolume', 'prostateweight', 
                       'age', 'hyperplasia', 'svi', 'capsular', 'gleason')
head(prostate)
```

# Analysis

## Building the initial model

First, we build a regression model with the required variables, and look at the summary statistics.

```{r lm}
prostate_lm = lm(psalevel ~ cancervolume + age + hyperplasia + svi + capsular + gleason,
                 data = prostate)
summary(prostate_lm)
```

## Manual model selection

We can see in the summary for the linear model that capsular penetration, i.e. `capsular` is the least significant variable since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var1}
prostate_lm_reduced_1 = lm(psalevel ~ cancervolume + age + hyperplasia + svi + gleason,
                         data = prostate)
summary(prostate_lm_reduced_1)
```

Again, we notice that patient age, i.e. `age` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var2}
prostate_lm_reduced_2 = lm(psalevel ~ cancervolume + hyperplasia + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_2)
```

Now, we notice that the amount of benign prostatic hyperplasia, i.e. `hyperplasia` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var3}
prostate_lm_reduced_3 = lm(psalevel ~ cancervolume + svi + gleason,
                           data = prostate)
summary(prostate_lm_reduced_3)
```

Finally, we notice that the Gleason score, i.e. `gleason` is the least significant since it has the highest *p*-value, which is over 0.05. We fit another linear model by removing this variable.

```{r rm_var4}
prostate_lm_reduced_4 = lm(psalevel ~ cancervolume + svi,
                           data = prostate)
summary(prostate_lm_reduced_4)
```

We see that all the variables in the model have a *p*-value less than 0.05, which means that their slopes are all statistically significant, and they contribute to explaining the variation in the response.

Thus, the final model we have is `psalevel =` `r prostate_lm_reduced_4$coefficients[1]` + `r prostate_lm_reduced_4$coefficients[2]` $\cdot$ `cancervolume` + `r prostate_lm_reduced_4$coefficients[3]` $\cdot$ `svi`.

\newpage

## Unusual observations

We will first look at identifying high leverage points. 
```{r high_leverage}
n = dim(prostate)[1] # Sample Size 
chosen_model = prostate_lm_reduced_4
p = length(variable.names(chosen_model)) # Predictors plus intercepts.

prostate_leverages = lm.influence(chosen_model)$hat

prostate_high_leverage = prostate_leverages[prostate_leverages > 2*p/n]
prostate_high_leverage



```
We now have the following high leverage points at our disposal.
We now need to find which of the following high leverage points can be considered "bad" high leverage points and which can be considered "good".
```{r}


  IQR_y = IQR(prostate$psalevel)

QT1_y = quantile(prostate$psalevel, 0.25)
QT3_y = quantile(prostate$psalevel, 0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y, upper_lim_y)
vector_lim_y

```
```{r}


```



## Model assumptions

